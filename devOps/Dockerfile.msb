# First Stage.  Build from CUDA development image
FROM nvidia/cuda:12.8.0-devel-ubuntu24.04 AS builder

ENV PYTHONUNBUFFERED=1 \
    PYTHONOPTIMIZE=1 \
    PIP_NO_CACHE_DIR=1 \
    FORCE_CMAKE=1 \
    KMP_DUPLICATE_LIB_OK=TRUE \
    PIP_INDEX_URL=https://pypi.org/simple \
    PIP_PYPI_URL=https://pypi.org/simple

RUN apt-get update && apt-get install -y --no-install-recommends \
    libomp-dev \
    python3.12 \
    python3-pip \
    build-essential \
    python3-dev \
    ninja-build \
    libclblast-dev \
    libomp-dev \
    wget \
    git && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1 && \
    update-alternatives --config python3 && \
    CMAKE_ARGS="-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=all-major" \
    pip3 install --debug --no-cache-dir --break-system-packages llama-cpp-python && \
    apt-get purge -y build-essential ninja-build python3-dev && \
    apt-get autoremove -y && \
    apt-get clean -y && \
    rm -rf /root/.cache /tmp/* /var/tmp/* /var/lib/apt/lists/*

# Base Image
FROM nvidia/cuda:12.8.0-base-ubuntu24.04

# Set environment variables for optimization
ENV PYTHONUNBUFFERED=1 \
    PYTHONOPTIMIZE=1 \
    PIP_NO_CACHE_DIR=1 \
    FORCE_CMAKE=1 \
    KMP_DUPLICATE_LIB_OK=TRUE \
    PIP_INDEX_URL=https://pypi.org/simple \
    PIP_PYPI_URL=https://pypi.org/simple \
    LD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/nvidia/cublas/lib:$LD_LIBRARY_PATH

# Copy and install Python dependencies
COPY requirements.txt requirements.txt

# Install dependencies and immediately clean up to reduce image size
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.12 \
    python3-pip \
    libjpeg-dev \
    zlib1g-dev \
    python3-dev \
    libomp-dev \
    libclblast-dev \
    libopenblas-dev \
    libomp-dev \
    libgomp1 \
    wget \
    git \
    curl && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1 && \
    update-alternatives --config python3 && \
    pip3 install --debug --no-cache-dir --verbose --break-system-packages --index-url https://download.pytorch.org/whl/cu126 torch && \
    pip3 install --debug --no-cache-dir --verbose --upgrade --break-system-packages --index-url \ 
    https://pypi.org/simple -r requirements.txt && \
    apt-get purge -y build-essential && \
    apt-get autoremove -y && \
    apt-get clean -y && \
    rm -rf /root/.cache /tmp/* /var/tmp/* /var/lib/apt/lists/*

# Copy llama-cpp libraries compiled with cuda from build local path
COPY --from=builder /usr/local/lib/python3.12/dist-packages/llama_cpp /usr/local/lib/python3.12/dist-packages/llama_cpp
COPY --from=builder  /usr/local/lib/python3.12/dist-packages/llama_cpp_python-0.3.7.dist-info /usr/local/lib/python3.12/dist-packages/llama_cpp_python-0.3.7.dist-info 
COPY --from=builder /usr/local/lib/python3.12/dist-packages/lib /usr/local/lib/python3.12/dist-packages/lib
COPY --from=builder /usr/local/lib/python3.12/dist-packages/include /usr/local/lib/python3.12/dist-packages/include

# Create model directory and download model
RUN mkdir -p /app/jinv3/modelCache

# Copy script and download model inside a single RUN layer
COPY saveModels.py saveModels.py
RUN python3 saveModels.py && \
    curl -o /app/Phi-3.5-mini-instruct-Q6_K.gguf -L \
    "https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q6_K.gguf" || \
    { echo "Failed to download LLM model" && exit 1; } && \
    apt-get autoremove -y && \
    apt-get clean -y && \
    rm -rf /root/.cache/* /tmp/* /var/tmp/* /var/lib/apt/lists/*
