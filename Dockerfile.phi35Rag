# Base Image
FROM nvidia/cuda:12.8.0-base-ubuntu24.04

# Set environment variables for optimization
# KMP_DUPLICATE_LIB_OK is set to avoid duplicate dylib issue , for now
ENV PYTHONUNBUFFERED=1 \
    PYTHONOPTIMIZE=1 \
    PIP_NO_CACHE_DIR=1 \
    FORCE_CMAKE=1 \
    KMP_DUPLICATE_LIB_OK=TRUE \
    CC=clang \
    CXX=clang++

# why clang : https://github.com/ggerganov/llama.cpp/issues/11428
# Install Clang and related packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    clang \
    lld \
    clang-format \
    llvm \
    libomp-dev \
    python3 \
    python3-pip \
    build-essential \
    ninja-build \
    libclblast-dev \
    libopenblas-dev \
    libomp-dev \
    libgomp1 \
    wget \
    git \
    curl && \
    ln -s /usr/bin/python3 /usr/bin/python && \
    apt-get clean && rm -rf /var/lib/apt/lists/*


<<<<<<< Updated upstream
<<<<<<< Updated upstream
# RUN python3 -m pip install --force-reinstall --upgrade --break-system-packages llama-cpp-python  --verbose

RUN pip install  --verbose --break-system-packages llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu125
=======
=======
>>>>>>> Stashed changes

# install llama cpp's python bindings
RUN CMAKE_ARGS="-DGGML_NATIVE=OFF -DCMAKE_CXX_FLAGS="-march=native" -DCMAKE_C_FLAGS="-march=native" -DGGML_CPU_ARM_ARCH=native" pip install  --verbose --break-system-packages llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cuda125
>>>>>>> Stashed changes

# list of required packages
COPY requirements.txt requirements.txt

<<<<<<< Updated upstream
<<<<<<< Updated upstream
# Install Python dependencies with prebuilt wheels for faster installation
RUN pip install --force-reinstall --upgrade --break-system-packages requests \
    beautifulsoup4 \
    rich \
    tiktoken \
    PyMuPDF \
    langchain \
    langchain-community \
    faiss-cpu 

# Set working directory
WORKDIR /app
=======
=======
>>>>>>> Stashed changes
# Install required packages
RUN pip install --no-cache-dir --upgrade  --break-system-packages -r requirements.txt

# Create a directory to save the model
RUN mkdir -p /app/jinv3

# Add a Python script that downloads the model
COPY saveModels.py saveModels.py

RUN python3 saveModels.py
>>>>>>> Stashed changes

# Download model files with robust error handling
RUN curl -o /app/all-MiniLM-L6-v2-ggml-model-f16.gguf -L "https://huggingface.co/second-state/All-MiniLM-L6-v2-Embedding-GGUF/resolve/main/all-MiniLM-L6-v2-ggml-model-f16.gguf" || \
    { echo "Failed to download Embedding model" && exit 1; } && \
    curl -o /app/Phi-3.5-mini-instruct-Q6_K.gguf -L "https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q6_K.gguf" || \
    { echo "Failed to download llm model" && exit 1; }


# Remove Clang, LLVM, and other build tools to reduce image size
RUN apt-get purge -y clang lld llvm build-essential && \
    apt-get autoremove -y && \
    apt-get clean -y && \
    rm -rf /root/.cache/* /tmp/* /var/tmp/*
<<<<<<< Updated upstream

# Unset CC and CXX since Clang is no longer available
ENV CC="" \
    CXX=""

=======
>>>>>>> Stashed changes

