# Base Image
FROM nvidia/cuda:12.8.0-base-ubuntu24.04

# Set environment variables for optimization
ENV PYTHONUNBUFFERED=1 \
    PYTHONOPTIMIZE=1 \
    PIP_NO_CACHE_DIR=1 \
    FORCE_CMAKE=1 \
    KMP_DUPLICATE_LIB_OK=TRUE 

# Install required packages for build
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-pip \
    libomp-dev libjpeg-dev zlib1g-dev python3-dev \
    ninja-build libclblast-dev libopenblas-dev \
    wget git curl && \
    rm -rf /var/lib/apt/lists/*

# Install llama-cpp-python (with cleanup)
RUN CMAKE_ARGS="-DGGML_NATIVE=OFF -DCMAKE_CXX_FLAGS='-march=native' -DCMAKE_C_FLAGS='-march=native' -DGGML_CPU_ARM_ARCH=native" \
    pip install --no-cache-dir --verbose --break-system-packages \
    llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cuda125

# Copy and install dependencies
COPY requirements.txt requirements.txt
RUN pip install --no-cache-dir --verbose --upgrade --break-system-packages -r requirements.txt

# Create a directory to save the model
RUN mkdir -p /app/jinv3/modelCache

# Copy script and download model inside a single RUN layer
COPY saveModels.py saveModels.py
RUN python3 saveModels.py && \
    curl -o /app/Phi-3.5-mini-instruct-Q6_K.gguf -L \
    "https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q6_K.gguf" || \
    { echo "Failed to download LLM model" && exit 1; }

# Install PyTorch (ensure compatibility)
RUN pip3 install --no-cache-dir --verbose --upgrade --break-system-packages \
    torch --index-url https://download.pytorch.org/whl/cu126

# Remove unnecessary build tools in the same step to reduce layers
RUN apt-get purge -y clang lld llvm && \
    apt-get autoremove -y && \
    apt-get clean -y && \
    rm -rf /root/.cache /tmp/* /var/tmp/* /var/lib/apt/lists/*
